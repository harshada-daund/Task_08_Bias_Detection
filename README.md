# Task 08 â€“ Bias Detection in LLM-Generated Data Narratives

## ðŸ“˜ Overview

I am currently in the **planning stage** for Task 08. This project will explore how **Large Language Models (LLMs)** might produce biased or inconsistent narratives when summarizing or interpreting the same dataset. I plan to use the **Syracuse Womenâ€™s Lacrosse 2024 dataset**, which has been anonymized using generic player labels (Player A, Player B, Player C).

---

## ðŸŽ¯ Purpose

The main goal is to design a small controlled experiment that tests whether changes in question framing or context â€” such as positive vs. negative wording or including demographic details â€” influence the responses produced by LLMs. This will help identify possible **framing**, **confirmation**, and **demographic biases** in AI-generated outputs.

---

## ðŸ§© Current Planning Progress

* Reviewed the official **Task 08** instructions and research objectives.
* Selected the **anonymized Syracuse Womenâ€™s Lacrosse dataset** for testing.
* Outlined initial ideas for prompts that vary by tone and framing.
* Planned to test results across different LLMs (ChatGPT, Claude, Gemini).
* Created the basic GitHub repository structure to store prompts, outputs, and analysis scripts.

No experiments have been run yet â€” this phase is focused on planning and setup only.

---

## ðŸ§  Next Steps

* Finalize prompt variations and bias hypotheses.
* Begin running model tests in the next reporting period.
* Log results systematically for analysis and comparison.

